Papers
    - Searching for the next best mate: Peter Todd
    - Should I stay or should I go: ANgela Wu? & Peter Dan
    - Don't stop till you get enough: Guerickis & Maloney
    - The value of approaching best things: Rich & Gureckis
        
Game Parameters
    - Number of cards 
    - Distribution of values
    - Range of values
    - Sampling with/out replacement
    - Search cost
        
Overall Ideas
    - Transfer learning: Choose a subset of games, train agents on the training games and evaluate on unseen games
    - Apply search cost for better human comparison
    - "How far off are we if we stop at X%"
    - Are there flat maximas?
        
Agents
    - Q-learning
        - Actual values
        - Greater than, less than, equal to mean/median
        - Hashed values    
    - SARSA
    - Deep Q-learning
    - Monte Carlo

============================================================

Transfer Learning Test Cases
Default: 50 cards, range [1, 1000], uniform distribution, no replacement, search cost 0

Altering Number of Cards
    - Default -> 5 cards, range [1, 1000], uniform distribution, no replacement, search cost 0
    - Default -> 51 cards, range [1, 1000], uniform distribution, no replacement, search cost 0
    - Default -> 500 cards, range [1, 1000], uniform distribution, no replacement, search cost 0

Altering Range
    - Default -> 50 cards, range [1, 100], uniform distribution, no replacement, search cost 0
    - Default -> 50 cards, range [1, 1001], uniform distribution, no replacement, search cost 0
    - Default -> 50 cards, range [1, 10000], uniform distribution, no replacement, search cost 0

Altering Replacement
    - Default -> 50 cards, range [1, 1000], uniform distribution, with replacement, search cost 0

Altering Search Cost
    - Default -> 50 cards, range [1, 1000], uniform distribution, no replacement, search cost 1

Altering Distribution